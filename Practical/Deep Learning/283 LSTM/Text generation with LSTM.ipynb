{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text generation with LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.2.4'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus length: 600901\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "\n",
    "path = keras.utils.get_file(\n",
    "    'nietzsche.txt',\n",
    "    origin='https://s3.amazonaws.com/text-datasets/nietzsche.txt')\n",
    "text = open(path).read().lower()\n",
    "print('Corpus length:', len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences: 200281\n",
      "Unique characters: 59\n",
      "Vectorization...\n"
     ]
    }
   ],
   "source": [
    "# Length of extracted character sequences\n",
    "maxlen = 60\n",
    "\n",
    "# We sample a new sequence every `step` characters\n",
    "step = 3\n",
    "\n",
    "# This holds our extracted sequences\n",
    "sentences = []\n",
    "\n",
    "# This holds the targets (the follow-up characters)\n",
    "next_chars = []\n",
    "\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "print('Number of sequences:', len(sentences))\n",
    "\n",
    "# List of unique characters in the corpus\n",
    "chars = sorted(list(set(text)))\n",
    "print('Unique characters:', len(chars))\n",
    "# Dictionary mapping unique characters to their index in `chars`\n",
    "char_indices = dict((char, chars.index(char)) for char in chars)\n",
    "\n",
    "# Next, one-hot encode the characters into binary arrays.\n",
    "print('Vectorization...')\n",
    "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        x[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras import layers\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(layers.LSTM(128, input_shape=(maxlen, len(chars))))\n",
    "model.add(layers.Dense(len(chars), activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "optimizer = keras.optimizers.RMSprop(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the language model and sampling from it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:2741: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "Epoch 1/1\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "200281/200281 [==============================] - 344s 2ms/step - loss: 2.0064\n",
      "--- Generating with seed: \"rceive with the utmost\n",
      "amazement that this state of self con\"\n",
      "------ temperature: 0.2\n",
      "rceive with the utmost\n",
      "amazement that this state of self consequent the sumpress of the such his a stand the self-deres the more the self-deres and the every to the such such is the sumperional the such the such a stinces of the surter the such his the such a stinces the contermine the such his is the soul the such a stand the are to the sumperitions of the will the has the such his is the such his the same the self-deres the will his reals the consequent \n",
      "------ temperature: 0.5\n",
      "s the same the self-deres the will his reals the consequent to the\n",
      "fage that the everythere is will he he deen the higher the interman with he langured to the itself of the\n",
      "course concertions its a concertained the sumprese the sterity and is the higher to man. the sense of the not of the has there as the man of the cometions the such his our higher is the suppiner with the will the same he chares, the were be one therefore that is the higher his concerver\n",
      "------ temperature: 1.0\n",
      ", the were be one therefore that is the higher his concerverathing heraeds his a finally mube a for his which is whichout of morar--dheal ensecoidi is to is his \n",
      "isherchute astheral, one, in be whatever, revertion to with dever, the look, which ter\n",
      "in will an who alvent, a\n",
      "preffieht of clurt for in his tome consignce thaid undomanes, highones. lest rased lenees, philosophites: , man--ss a ckrit of chins, theuling the which oul persencemunt obintrodmation f\n",
      "------ temperature: 1.2\n",
      " chins, theuling the which oul persencemunt obintrodmation foom\n",
      "coneexposeny, rolonk\n",
      " is for yartith\n",
      "swithctan\n",
      "a confition as\n",
      "it\n",
      "lensscarly. it pranescicoupane everild, ban\n",
      "empinates everytever hatter and wether of ny dive men be a his yot, tannfungly his comram ent manys evedyeciesly, gerbent badents, althish awilfur it is oothate,\n",
      "an criderocal tomatwing comminity and\n",
      "an has\n",
      "ampreins\n",
      "is \"treaves\n",
      "himself'itions,\"\n",
      "the heresgess who long, eavied is teause, \n",
      "epoch 2\n",
      "Epoch 1/1\n",
      "200281/200281 [==============================] - 301s 2ms/step - loss: 1.6565\n",
      "--- Generating with seed: \"avior of the obstinate man who\n",
      "says \"i would rather be shot \"\n",
      "------ temperature: 0.2\n",
      "avior of the obstinate man who\n",
      "says \"i would rather be shot the spirit and artion the soul and foreurs the strength in the strength of the strength of the surted that the sense of the discovered and such one of the soul is as in the self--and all the soul in the sense of the the strength of the soul and all the soul the action of the strength, and as the sense of the the such and and all the soul and and soul and the sense of the something the soul and and\n",
      "------ temperature: 0.5\n",
      "and and soul and the sense of the something the soul and and the fary and sacried and original foreloves the possible,\n",
      "as the conscience of the state they are any the right and but one the longless of the spiret in order the any it is the strength and really putsic perception as it is a doust and properation of the conscuented and all and still distruted in a as i course of the findation and something and orselves, when the sumperismed to the problem of th\n",
      "------ temperature: 1.0\n",
      "hing and orselves, when the sumperismed to the problem of though,\n",
      "commoning cidlationedmentery aupter bethipry moneous orn necess hado: an the perhaps and throse\n",
      "one thar if the sons overself, ir!inerance, to\n",
      "the ourselves\n",
      "ussaanstatedo of of\n",
      "orn and philosophy, and\n",
      "redigions as the develne for the parlities conduming itseenan, \"the sighted they is not damilliar,\"\n",
      "and concreling they perhaps hand of\n",
      "us, all relong 4(any generation of resistion of hower eve\n",
      "------ temperature: 1.2\n",
      "of\n",
      "us, all relong 4(any generation of resistion of hower ever looks\n",
      "ofdess: indignish: german in, augh has nacuotion\n",
      "of natural eus afcerunt are =chinition: \"as his wansed inslorms tranch\n",
      "nornonain . in old\n",
      "\"mulaphith of repripte--that the goody, from the prompor. boothr, tett uses?\n",
      "\n",
      "\n",
      "     \n",
      ".   toces from sulfournedentlester,\n",
      "to be domalotion, consuct of\n",
      "maddicte crasmorl. is ad act agr dove?\"\" sobuequani in\n",
      "thas midstered\n",
      "\"whai, and\n",
      "correwine and \"philoso\n",
      "epoch 3\n",
      "Epoch 1/1\n",
      "200281/200281 [==============================] - 285s 1ms/step - loss: 1.5672\n",
      "--- Generating with seed: \"eto is recent, initial, awkward, and coarse-fingered:--an\n",
      "in\"\n",
      "------ temperature: 0.2\n",
      "eto is recent, initial, awkward, and coarse-fingered:--an\n",
      "into the strongers of himself and all the moral and all the strongers of the strongers and a provest and all the stronger the present the stronger of the strength and all the strength of all th"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import sys\n",
    "\n",
    "for epoch in range(1, 60):\n",
    "    print('epoch', epoch)\n",
    "    # Fit the model for 1 epoch on the available training data\n",
    "    model.fit(x, y,\n",
    "              batch_size=128,\n",
    "              epochs=1)\n",
    "\n",
    "    # Select a text seed at random\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "    generated_text = text[start_index: start_index + maxlen]\n",
    "    print('--- Generating with seed: \"' + generated_text + '\"')\n",
    "\n",
    "    for temperature in [0.2, 0.5, 1.0, 1.2]:\n",
    "        print('------ temperature:', temperature)\n",
    "        sys.stdout.write(generated_text)\n",
    "\n",
    "        # We generate 400 characters\n",
    "        for i in range(400):\n",
    "            sampled = np.zeros((1, maxlen, len(chars)))\n",
    "            for t, char in enumerate(generated_text):\n",
    "                sampled[0, t, char_indices[char]] = 1.\n",
    "\n",
    "            preds = model.predict(sampled, verbose=0)[0]\n",
    "            next_index = sample(preds, temperature)\n",
    "            next_char = chars[next_index]\n",
    "\n",
    "            generated_text += next_char\n",
    "            generated_text = generated_text[1:]\n",
    "\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
