{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Diabetes Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Diabetes Dataset involves predicting the onset of diabetes within 5 years in given medical details.\n",
    "\n",
    "    Dataset File.\n",
    "    Dataset Details\n",
    "\n",
    "It is a binary (2-class) classification problem. The number of observations for each class is not balanced. There are 768 observations with 8 input variables and 1 output variable. The variable names are as follows:\n",
    "\n",
    "    0. Number of times pregnant.\n",
    "    1. Plasma glucose concentration a 2 hours in an oral glucose tolerance test.\n",
    "    2. Diastolic blood pressure (mm Hg).\n",
    "    3. Triceps skinfold thickness (mm).\n",
    "    4. 2-Hour serum insulin (mu U/ml).\n",
    "    5. Body mass index (weight in kg/(height in m)^2).\n",
    "    6. Diabetes pedigree function.\n",
    "    7. Age (years).\n",
    "    8. Class variable (0 or 1).\n",
    "\n",
    "The baseline performance of predicting the most prevalent class is a classification accuracy of approximately 65%. Top results achieve a classification accuracy of approximately 77%."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "6,148,72,35,0,33.6,0.627,50,1\n",
    "1,85,66,29,0,26.6,0.351,31,0\n",
    "8,183,64,0,0,23.3,0.672,32,1\n",
    "1,89,66,23,94,28.1,0.167,21,0\n",
    "0,137,40,35,168,43.1,2.288,33,1\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset is known to have missing values.\n",
    "\n",
    "Specifically, there are missing observations for some columns that are marked as a zero value.\n",
    "\n",
    "We can corroborate this by the definition of those columns and the domain knowledge that a zero value is invalid for those measures, e.g. a zero for body mass index or blood pressure is invalid.\n",
    "\n",
    "Download the dataset from here and save it to your current working directory with the file name pima-indians-diabetes.csv .\n",
    "\n",
    "    pima-indians-diabetes.csv\n",
    "    https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.csv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Mark Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will look at how we can identify and mark values as missing.\n",
    "\n",
    "We can use plots and summary statistics to help identify missing or corrupt data.\n",
    "\n",
    "We can load the dataset as a Pandas DataFrame and print summary statistics on each attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                0           1           2           3           4           5  \\\n",
      "count  768.000000  768.000000  768.000000  768.000000  768.000000  768.000000   \n",
      "mean     3.845052  120.894531   69.105469   20.536458   79.799479   31.992578   \n",
      "std      3.369578   31.972618   19.355807   15.952218  115.244002    7.884160   \n",
      "min      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
      "25%      1.000000   99.000000   62.000000    0.000000    0.000000   27.300000   \n",
      "50%      3.000000  117.000000   72.000000   23.000000   30.500000   32.000000   \n",
      "75%      6.000000  140.250000   80.000000   32.000000  127.250000   36.600000   \n",
      "max     17.000000  199.000000  122.000000   99.000000  846.000000   67.100000   \n",
      "\n",
      "                6           7           8  \n",
      "count  768.000000  768.000000  768.000000  \n",
      "mean     0.471876   33.240885    0.348958  \n",
      "std      0.331329   11.760232    0.476951  \n",
      "min      0.078000   21.000000    0.000000  \n",
      "25%      0.243750   24.000000    0.000000  \n",
      "50%      0.372500   29.000000    0.000000  \n",
      "75%      0.626250   41.000000    1.000000  \n",
      "max      2.420000   81.000000    1.000000  \n"
     ]
    }
   ],
   "source": [
    "# load and summarize the dataset\n",
    "from pandas import read_csv\n",
    "\n",
    "# load the dataset\n",
    "dataset = read_csv(\"pima-indians-diabetes.csv\", header=None)\n",
    "# summarize the dataset\n",
    "print(dataset.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is useful.\n",
    "\n",
    "We can see that there are columns that have a minimum value of zero (0). On some columns, a value of zero does not make sense and indicates an invalid or missing value.\n",
    "\n",
    "Specifically, the following columns have an invalid zero minimum value:\n",
    "\n",
    "    1: Plasma glucose concentration\n",
    "    2: Diastolic blood pressure\n",
    "    3: Triceps skinfold thickness\n",
    "    4: 2-Hour serum insulin\n",
    "    5: Body mass index\n",
    "\n",
    "Let’s confirm this my looking at the raw data, the example prints the first 20 rows of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0    1   2   3    4     5      6   7  8\n",
      "0    6  148  72  35    0  33.6  0.627  50  1\n",
      "1    1   85  66  29    0  26.6  0.351  31  0\n",
      "2    8  183  64   0    0  23.3  0.672  32  1\n",
      "3    1   89  66  23   94  28.1  0.167  21  0\n",
      "4    0  137  40  35  168  43.1  2.288  33  1\n",
      "5    5  116  74   0    0  25.6  0.201  30  0\n",
      "6    3   78  50  32   88  31.0  0.248  26  1\n",
      "7   10  115   0   0    0  35.3  0.134  29  0\n",
      "8    2  197  70  45  543  30.5  0.158  53  1\n",
      "9    8  125  96   0    0   0.0  0.232  54  1\n",
      "10   4  110  92   0    0  37.6  0.191  30  0\n",
      "11  10  168  74   0    0  38.0  0.537  34  1\n",
      "12  10  139  80   0    0  27.1  1.441  57  0\n",
      "13   1  189  60  23  846  30.1  0.398  59  1\n",
      "14   5  166  72  19  175  25.8  0.587  51  1\n",
      "15   7  100   0   0    0  30.0  0.484  32  1\n",
      "16   0  118  84  47  230  45.8  0.551  31  1\n",
      "17   7  107  74   0    0  29.6  0.254  31  1\n",
      "18   1  103  30  38   83  43.3  0.183  33  0\n",
      "19   1  115  70  30   96  34.6  0.529  32  1\n"
     ]
    }
   ],
   "source": [
    "# load the dataset and review rows\n",
    "from pandas import read_csv\n",
    "\n",
    "# load the dataset\n",
    "dataset = read_csv(\"pima-indians-diabetes.csv\", header=None)\n",
    "# print the first 20 rows of data\n",
    "print(dataset.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get a count of the number of missing values on each of these columns. We can do this my marking all of the values in the subset of the DataFrame we are interested in that have zero values as True. We can then count the number of true values in each column.\n",
    "\n",
    "We can do this my marking all of the values in the subset of the DataFrame we are interested in that have zero values as True. We can then count the number of true values in each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1      5\n",
      "2     35\n",
      "3    227\n",
      "4    374\n",
      "5     11\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# example of summarizing the number of missing values for each variable\n",
    "from pandas import read_csv\n",
    "\n",
    "# load the dataset\n",
    "dataset = read_csv(\"pima-indians-diabetes.csv\", header=None)\n",
    "# count the number of missing values for each column\n",
    "num_missing = (dataset[[1, 2, 3, 4, 5]] == 0).sum()\n",
    "# report the results\n",
    "print(num_missing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that columns 1,2 and 5 have just a few zero values, whereas columns 3 and 4 show a lot more, nearly half of the rows.\n",
    "\n",
    "This highlights that different “missing value” strategies may be needed for different columns, e.g. to ensure that there are still a sufficient number of records left to train a predictive model.\n",
    "\n",
    "In Python, specifically Pandas, NumPy and Scikit-Learn, we mark missing values as NaN.\n",
    "\n",
    "Values with a NaN value are ignored from operations like sum, count, etc.\n",
    "\n",
    "We can mark values as NaN easily with the Pandas DataFrame by using the replace() function on a subset of the columns we are interested in.\n",
    "\n",
    "After we have marked the missing values, we can use the isnull() function to mark all of the NaN values in the dataset as True and get a count of the missing values for each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      0\n",
      "1      5\n",
      "2     35\n",
      "3    227\n",
      "4    374\n",
      "5     11\n",
      "6      0\n",
      "7      0\n",
      "8      0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# example of marking missing values with nan values\n",
    "from numpy import nan\n",
    "from pandas import read_csv\n",
    "\n",
    "# load the dataset\n",
    "dataset = read_csv(\"pima-indians-diabetes.csv\", header=None)\n",
    "# replace '0' values with 'nan'\n",
    "dataset[[1, 2, 3, 4, 5]] = dataset[[1, 2, 3, 4, 5]].replace(0, nan)\n",
    "# count the number of nan values in each column\n",
    "print(dataset.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the same example, except we print the first 20 rows of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0      1     2     3      4     5      6   7  8\n",
      "0    6  148.0  72.0  35.0    NaN  33.6  0.627  50  1\n",
      "1    1   85.0  66.0  29.0    NaN  26.6  0.351  31  0\n",
      "2    8  183.0  64.0   NaN    NaN  23.3  0.672  32  1\n",
      "3    1   89.0  66.0  23.0   94.0  28.1  0.167  21  0\n",
      "4    0  137.0  40.0  35.0  168.0  43.1  2.288  33  1\n",
      "5    5  116.0  74.0   NaN    NaN  25.6  0.201  30  0\n",
      "6    3   78.0  50.0  32.0   88.0  31.0  0.248  26  1\n",
      "7   10  115.0   NaN   NaN    NaN  35.3  0.134  29  0\n",
      "8    2  197.0  70.0  45.0  543.0  30.5  0.158  53  1\n",
      "9    8  125.0  96.0   NaN    NaN   NaN  0.232  54  1\n",
      "10   4  110.0  92.0   NaN    NaN  37.6  0.191  30  0\n",
      "11  10  168.0  74.0   NaN    NaN  38.0  0.537  34  1\n",
      "12  10  139.0  80.0   NaN    NaN  27.1  1.441  57  0\n",
      "13   1  189.0  60.0  23.0  846.0  30.1  0.398  59  1\n",
      "14   5  166.0  72.0  19.0  175.0  25.8  0.587  51  1\n",
      "15   7  100.0   NaN   NaN    NaN  30.0  0.484  32  1\n",
      "16   0  118.0  84.0  47.0  230.0  45.8  0.551  31  1\n",
      "17   7  107.0  74.0   NaN    NaN  29.6  0.254  31  1\n",
      "18   1  103.0  30.0  38.0   83.0  43.3  0.183  33  0\n",
      "19   1  115.0  70.0  30.0   96.0  34.6  0.529  32  1\n"
     ]
    }
   ],
   "source": [
    "# example of review rows from the dataset with missing values marked\n",
    "from numpy import nan\n",
    "from pandas import read_csv\n",
    "\n",
    "# load the dataset\n",
    "dataset = read_csv(\"pima-indians-diabetes.csv\", header=None)\n",
    "# replace '0' values with 'nan'\n",
    "dataset[[1, 2, 3, 4, 5]] = dataset[[1, 2, 3, 4, 5]].replace(0, nan)\n",
    "# print the first 20 rows of data\n",
    "print(dataset.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Missing Values Causes Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having missing values in a dataset can cause errors with some machine learning algorithms.\n",
    "\n",
    "In this section, we will try to evaluate a the Linear Discriminant Analysis (LDA) algorithm on the dataset with missing values.\n",
    "\n",
    "This is an algorithm that does not work when there are missing values in the dataset.\n",
    "\n",
    "The below example marks the missing values in the dataset, as we did in the previous section, then attempts to evaluate LDA using 3-fold cross validation and print the mean accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "\nAll the 3 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n3 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/opt/python/3.10.4/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/opt/python/3.10.4/lib/python3.10/site-packages/sklearn/discriminant_analysis.py\", line 550, in fit\n    X, y = self._validate_data(\n  File \"/opt/python/3.10.4/lib/python3.10/site-packages/sklearn/base.py\", line 596, in _validate_data\n    X, y = check_X_y(X, y, **check_params)\n  File \"/opt/python/3.10.4/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 1074, in check_X_y\n    X = check_array(\n  File \"/opt/python/3.10.4/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 899, in check_array\n    _assert_all_finite(\n  File \"/opt/python/3.10.4/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 146, in _assert_all_finite\n    raise ValueError(msg_err)\nValueError: Input X contains NaN.\nLinearDiscriminantAnalysis does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/workspaces/AI-ML-Course-Qmansys/Practical/Machine Learning/71 Dealing with Missing Data/Dealing with Missing Data.ipynb Cell 18\u001b[0m in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bchirag127-ai-ml-course-qmansys-v5ggp49j54jfwq6r/workspaces/AI-ML-Course-Qmansys/Practical/Machine%20Learning/71%20Dealing%20with%20Missing%20Data/Dealing%20with%20Missing%20Data.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m cv \u001b[39m=\u001b[39m KFold(n_splits\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, random_state\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bchirag127-ai-ml-course-qmansys-v5ggp49j54jfwq6r/workspaces/AI-ML-Course-Qmansys/Practical/Machine%20Learning/71%20Dealing%20with%20Missing%20Data/Dealing%20with%20Missing%20Data.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39m# evaluate the model\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://codespaces%2Bchirag127-ai-ml-course-qmansys-v5ggp49j54jfwq6r/workspaces/AI-ML-Course-Qmansys/Practical/Machine%20Learning/71%20Dealing%20with%20Missing%20Data/Dealing%20with%20Missing%20Data.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m result \u001b[39m=\u001b[39m cross_val_score(model, X, y, cv\u001b[39m=\u001b[39;49mcv, scoring\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39maccuracy\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bchirag127-ai-ml-course-qmansys-v5ggp49j54jfwq6r/workspaces/AI-ML-Course-Qmansys/Practical/Machine%20Learning/71%20Dealing%20with%20Missing%20Data/Dealing%20with%20Missing%20Data.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39m# report the mean performance\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bchirag127-ai-ml-course-qmansys-v5ggp49j54jfwq6r/workspaces/AI-ML-Course-Qmansys/Practical/Machine%20Learning/71%20Dealing%20with%20Missing%20Data/Dealing%20with%20Missing%20Data.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mAccuracy: \u001b[39m\u001b[39m%.3f\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m result\u001b[39m.\u001b[39mmean())\n",
      "File \u001b[0;32m/opt/python/3.10.4/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:515\u001b[0m, in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[39m# To ensure multimetric format is not supported\u001b[39;00m\n\u001b[1;32m    513\u001b[0m scorer \u001b[39m=\u001b[39m check_scoring(estimator, scoring\u001b[39m=\u001b[39mscoring)\n\u001b[0;32m--> 515\u001b[0m cv_results \u001b[39m=\u001b[39m cross_validate(\n\u001b[1;32m    516\u001b[0m     estimator\u001b[39m=\u001b[39;49mestimator,\n\u001b[1;32m    517\u001b[0m     X\u001b[39m=\u001b[39;49mX,\n\u001b[1;32m    518\u001b[0m     y\u001b[39m=\u001b[39;49my,\n\u001b[1;32m    519\u001b[0m     groups\u001b[39m=\u001b[39;49mgroups,\n\u001b[1;32m    520\u001b[0m     scoring\u001b[39m=\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39mscore\u001b[39;49m\u001b[39m\"\u001b[39;49m: scorer},\n\u001b[1;32m    521\u001b[0m     cv\u001b[39m=\u001b[39;49mcv,\n\u001b[1;32m    522\u001b[0m     n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[1;32m    523\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m    524\u001b[0m     fit_params\u001b[39m=\u001b[39;49mfit_params,\n\u001b[1;32m    525\u001b[0m     pre_dispatch\u001b[39m=\u001b[39;49mpre_dispatch,\n\u001b[1;32m    526\u001b[0m     error_score\u001b[39m=\u001b[39;49merror_score,\n\u001b[1;32m    527\u001b[0m )\n\u001b[1;32m    528\u001b[0m \u001b[39mreturn\u001b[39;00m cv_results[\u001b[39m\"\u001b[39m\u001b[39mtest_score\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m/opt/python/3.10.4/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:285\u001b[0m, in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    265\u001b[0m parallel \u001b[39m=\u001b[39m Parallel(n_jobs\u001b[39m=\u001b[39mn_jobs, verbose\u001b[39m=\u001b[39mverbose, pre_dispatch\u001b[39m=\u001b[39mpre_dispatch)\n\u001b[1;32m    266\u001b[0m results \u001b[39m=\u001b[39m parallel(\n\u001b[1;32m    267\u001b[0m     delayed(_fit_and_score)(\n\u001b[1;32m    268\u001b[0m         clone(estimator),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    282\u001b[0m     \u001b[39mfor\u001b[39;00m train, test \u001b[39min\u001b[39;00m cv\u001b[39m.\u001b[39msplit(X, y, groups)\n\u001b[1;32m    283\u001b[0m )\n\u001b[0;32m--> 285\u001b[0m _warn_or_raise_about_fit_failures(results, error_score)\n\u001b[1;32m    287\u001b[0m \u001b[39m# For callabe scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[39m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[1;32m    289\u001b[0m \u001b[39m# the correct key.\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \u001b[39mif\u001b[39;00m callable(scoring):\n",
      "File \u001b[0;32m/opt/python/3.10.4/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:367\u001b[0m, in \u001b[0;36m_warn_or_raise_about_fit_failures\u001b[0;34m(results, error_score)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[39mif\u001b[39;00m num_failed_fits \u001b[39m==\u001b[39m num_fits:\n\u001b[1;32m    361\u001b[0m     all_fits_failed_message \u001b[39m=\u001b[39m (\n\u001b[1;32m    362\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mAll the \u001b[39m\u001b[39m{\u001b[39;00mnum_fits\u001b[39m}\u001b[39;00m\u001b[39m fits failed.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    363\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mIt is very likely that your model is misconfigured.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    364\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mYou can try to debug the error by setting error_score=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mraise\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    365\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBelow are more details about the failures:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mfit_errors_summary\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    366\u001b[0m     )\n\u001b[0;32m--> 367\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(all_fits_failed_message)\n\u001b[1;32m    369\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    370\u001b[0m     some_fits_failed_message \u001b[39m=\u001b[39m (\n\u001b[1;32m    371\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mnum_failed_fits\u001b[39m}\u001b[39;00m\u001b[39m fits failed out of a total of \u001b[39m\u001b[39m{\u001b[39;00mnum_fits\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    372\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThe score on these train-test partitions for these parameters\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    376\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBelow are more details about the failures:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mfit_errors_summary\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    377\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: \nAll the 3 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n3 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/opt/python/3.10.4/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/opt/python/3.10.4/lib/python3.10/site-packages/sklearn/discriminant_analysis.py\", line 550, in fit\n    X, y = self._validate_data(\n  File \"/opt/python/3.10.4/lib/python3.10/site-packages/sklearn/base.py\", line 596, in _validate_data\n    X, y = check_X_y(X, y, **check_params)\n  File \"/opt/python/3.10.4/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 1074, in check_X_y\n    X = check_array(\n  File \"/opt/python/3.10.4/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 899, in check_array\n    _assert_all_finite(\n  File \"/opt/python/3.10.4/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 146, in _assert_all_finite\n    raise ValueError(msg_err)\nValueError: Input X contains NaN.\nLinearDiscriminantAnalysis does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values\n"
     ]
    }
   ],
   "source": [
    "# example where missing values cause errors\n",
    "from numpy import nan\n",
    "from pandas import read_csv\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# load the dataset\n",
    "dataset = read_csv(\"pima-indians-diabetes.csv\", header=None)\n",
    "# replace '0' values with 'nan'\n",
    "dataset[[1, 2, 3, 4, 5]] = dataset[[1, 2, 3, 4, 5]].replace(0, nan)\n",
    "# split dataset into inputs and outputs\n",
    "values = dataset.values\n",
    "X = values[:, 0:8]\n",
    "y = values[:, 8]\n",
    "# define the model\n",
    "model = LinearDiscriminantAnalysis()\n",
    "# define the model evaluation procedure\n",
    "cv = KFold(n_splits=3, shuffle=True, random_state=1)\n",
    "# evaluate the model\n",
    "result = cross_val_score(model, X, y, cv=cv, scoring=\"accuracy\")\n",
    "# report the mean performance\n",
    "print(\"Accuracy: %.3f\" % result.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is as we expect.\n",
    "\n",
    "We are prevented from evaluating an LDA algorithm (and other algorithms) on the dataset with missing values.\n",
    "\n",
    "Now, we can look at methods to handle the missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Remove Rows With Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest strategy for handling missing data is to remove records that contain a missing value.\n",
    "\n",
    "We can do this by creating a new Pandas DataFrame with the rows containing missing values removed.\n",
    "\n",
    "Pandas provides the dropna() function that can be used to drop either columns or rows with missing data. We can use dropna() to remove all rows with missing data, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768, 9)\n",
      "(392, 9)\n"
     ]
    }
   ],
   "source": [
    "# example of removing rows that contain missing values\n",
    "from numpy import nan\n",
    "from pandas import read_csv\n",
    "\n",
    "# load the dataset\n",
    "dataset = read_csv(\"pima-indians-diabetes.csv\", header=None)\n",
    "# summarize the shape of the raw data\n",
    "print(dataset.shape)\n",
    "# replace '0' values with 'nan'\n",
    "dataset[[1, 2, 3, 4, 5]] = dataset[[1, 2, 3, 4, 5]].replace(0, nan)\n",
    "# drop rows with missing values\n",
    "dataset.dropna(inplace=True)\n",
    "# summarize the shape of the data with missing rows removed\n",
    "print(dataset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a dataset that we could use to evaluate an algorithm sensitive to missing values like LDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.781\n"
     ]
    }
   ],
   "source": [
    "# evaluate model on data after rows with missing data are removed\n",
    "from numpy import nan\n",
    "from pandas import read_csv\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# load the dataset\n",
    "dataset = read_csv(\"pima-indians-diabetes.csv\", header=None)\n",
    "# replace '0' values with 'nan'\n",
    "dataset[[1, 2, 3, 4, 5]] = dataset[[1, 2, 3, 4, 5]].replace(0, nan)\n",
    "# drop rows with missing values\n",
    "dataset.dropna(inplace=True)\n",
    "# split dataset into inputs and outputs\n",
    "values = dataset.values\n",
    "X = values[:, 0:8]\n",
    "y = values[:, 8]\n",
    "# define the model\n",
    "model = LinearDiscriminantAnalysis()\n",
    "# define the model evaluation procedure\n",
    "cv = KFold(n_splits=3, shuffle=True, random_state=1)\n",
    "# evaluate the model\n",
    "result = cross_val_score(model, X, y, cv=cv, scoring=\"accuracy\")\n",
    "# report the mean performance\n",
    "print(\"Accuracy: %.3f\" % result.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing rows with missing values can be too limiting on some predictive modeling problems, an alternative is to impute missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Impute Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imputing refers to using a model to replace missing values.\n",
    "\n",
    "There are many options we could consider when replacing a missing value, for example:\n",
    "\n",
    "    A constant value that has meaning within the domain, such as 0, distinct from all other values.\n",
    "    A value from another randomly selected record.\n",
    "    A mean, median or mode value for the column.\n",
    "    A value estimated by another predictive model.\n",
    "\n",
    "Any imputing performed on the training dataset will have to be performed on new data in the future when predictions are needed from the finalized model. This needs to be taken into consideration when choosing how to impute the missing values.\n",
    "\n",
    "For example, if you choose to impute with mean column values, these mean column values will need to be stored to file for later use on new data that has missing values.\n",
    "\n",
    "Pandas provides the fillna() function for replacing missing values with a specific value.\n",
    "\n",
    "For example, we can use fillna() to replace missing values with the mean value for each column, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0\n",
      "1    0\n",
      "2    0\n",
      "3    0\n",
      "4    0\n",
      "5    0\n",
      "6    0\n",
      "7    0\n",
      "8    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# manually impute missing values with numpy\n",
    "from pandas import read_csv\n",
    "from numpy import nan\n",
    "\n",
    "# load the dataset\n",
    "dataset = read_csv(\"pima-indians-diabetes.csv\", header=None)\n",
    "# mark zero values as missing or NaN\n",
    "dataset[[1, 2, 3, 4, 5]] = dataset[[1, 2, 3, 4, 5]].replace(0, nan)\n",
    "# fill missing values with mean column values\n",
    "dataset.fillna(dataset.mean(), inplace=True)\n",
    "# count the number of NaN values in each column\n",
    "print(dataset.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scikit-learn library provides the SimpleImputer pre-processing class that can be used to replace missing values.\n",
    "\n",
    "It is a flexible class that allows you to specify the value to replace (it can be something other than NaN) and the technique used to replace it (such as mean, median, or mode). The SimpleImputer class operates directly on the NumPy array instead of the DataFrame.\n",
    "\n",
    "The example below uses the SimpleImputer class to replace missing values with the mean of each column then prints the number of NaN values in the transformed matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing: 0\n"
     ]
    }
   ],
   "source": [
    "# example of imputing missing values using scikit-learn\n",
    "from numpy import nan\n",
    "from numpy import isnan\n",
    "from pandas import read_csv\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# load the dataset\n",
    "dataset = read_csv(\"pima-indians-diabetes.csv\", header=None)\n",
    "# mark zero values as missing or NaN\n",
    "dataset[[1, 2, 3, 4, 5]] = dataset[[1, 2, 3, 4, 5]].replace(0, nan)\n",
    "# retrieve the numpy array\n",
    "values = dataset.values\n",
    "# define the imputer\n",
    "imputer = SimpleImputer(missing_values=nan, strategy=\"mean\")\n",
    "# transform the dataset\n",
    "transformed_values = imputer.fit_transform(values)\n",
    "# count the number of NaN values in each column\n",
    "print(\"Missing: %d\" % isnan(transformed_values).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In either case, we can train algorithms sensitive to NaN values in the transformed dataset, such as LDA.\n",
    "\n",
    "The example below shows the LDA algorithm trained in the SimpleImputer transformed dataset.\n",
    "\n",
    "We use a Pipeline to define the modeling pipeline, where data is first passed through the imputer transform, then provided to the model. This ensures that the imputer and model are both fit only on the training dataset and evaluated on the test dataset within each cross-validation fold. This is important to avoid data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.762\n"
     ]
    }
   ],
   "source": [
    "# example of evaluating a model after an imputer transform\n",
    "from numpy import nan\n",
    "from pandas import read_csv\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "dataset = read_csv(\"pima-indians-diabetes.csv\", header=None)\n",
    "# mark zero values as missing or NaN\n",
    "dataset[[1, 2, 3, 4, 5]] = dataset[[1, 2, 3, 4, 5]].replace(0, nan)\n",
    "# split dataset into inputs and outputs\n",
    "values = dataset.values\n",
    "X = values[:, 0:8]\n",
    "y = values[:, 8]\n",
    "# define the imputer\n",
    "imputer = SimpleImputer(missing_values=nan, strategy=\"mean\")\n",
    "# define the model\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "# define the modeling pipeline\n",
    "pipeline = Pipeline(steps=[(\"imputer\", imputer), (\"model\", lda)])\n",
    "# define the cross validation procedure\n",
    "kfold = KFold(n_splits=3, shuffle=True, random_state=1)\n",
    "# evaluate the model\n",
    "result = cross_val_score(pipeline, X, y, cv=kfold, scoring=\"accuracy\")\n",
    "# report the mean performance\n",
    "print(\"Accuracy: %.3f\" % result.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try replacing the missing values with other values and see if you can lift the performance of the model.\n",
    "\n",
    "Maybe missing values have meaning in the data.\n",
    "\n",
    "Next we will look at using algorithms that treat missing values as just another value when modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Algorithms that Support Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not all algorithms fail when there is missing data.\n",
    "\n",
    "There are algorithms that can be made robust to missing data, such as k-Nearest Neighbors that can ignore a column from a distance measure when a value is missing.\n",
    "\n",
    "There are also algorithms that can use the missing value as a unique and different value when building the predictive model, such as classification and regression trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "4f946df053fbf2b937619d3c5458e7af74262f9a954d8797ba0b27400bcafe06"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
